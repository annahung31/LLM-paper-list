# Oral

prompting
[Tree of Thoughts: Deliberate Problem Solving with Large Language Models](https://neurips.cc/virtual/2023/oral/73874)
[Why think step by step? Reasoning emerges from the locality of experience](https://neurips.cc/virtual/2023/oral/73821)

efficient training
[Fine-Tuning Language Models with Just Forward Passes](https://neurips.cc/virtual/2023/oral/73844)
[QLoRA: Efficient Finetuning of Quantized LLMs](https://neurips.cc/virtual/2023/oral/73855)
[Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture](https://neurips.cc/virtual/2023/oral/73841)

### rlhf
[Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://neurips.cc/virtual/2023/oral/73865)
[Task Arithmetic in the Tangent Space: Improved Editing of Pre-Trained Models](https://neurips.cc/virtual/2023/oral/73879)
[Bridging RL Theory and Practice with the Effective Horizon](https://neurips.cc/virtual/2023/oral/73859)
[Are Emergent Abilities of Large Language Models a Mirage?](https://neurips.cc/virtual/2023/oral/73863)
[Siamese Masked Autoencoders](https://neurips.cc/virtual/2023/oral/73813)

### data?
[DataComp: In search of the next generation of multimodal datasets](https://neurips.cc/virtual/2023/oral/73739)

[Scaling Data-Constrained Language Models](https://neurips.cc/virtual/2023/oral/73832)
[OpenAssistant Conversations - Democratizing Large Language Model Alignment](https://neurips.cc/virtual/2023/oral/73741)

RAG
[LeanDojo: Theorem Proving with Retrieval-Augmented Language Models](https://neurips.cc/virtual/2023/oral/73738)

knowledge graph
[How to Turn Your Knowledge Graph Embeddings into Generative Models](https://neurips.cc/virtual/2023/oral/73848)

### combinatorial problem
[Optimizing Solution-Samplers for Combinatorial Problems: The Landscape of Policy-Gradient Method](https://neurips.cc/virtual/2023/oral/73826)

[Bridging Discrete and Backpropagation: Straight-Through and Beyond](https://neurips.cc/virtual/2023/oral/73827)

### multi modal 
# LLaVA: Large Language and Vision Assistant

### Visual Instruction Tuning
[LLaVA (llava-vl.github.io)](https://llava-vl.github.io/)

Tool use
[Toolformer: Language Models Can Teach Themselves to Use Tools](https://neurips.cc/virtual/2023/oral/73843)
[ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings](https://neurips.cc/virtual/2023/oral/73868)


CV
[Understanding Diffusion Objectives as the ELBO with Simple Data Augmentation](https://neurips.cc/virtual/2023/oral/73857)



## spotlight
[MetaBox: A Benchmark Platform for Meta-Black-Box Optimization with Reinforcement Learning](https://neurips.cc/virtual/2023/poster/73497)


equivariant 
 [Clifford Group Equivariant Neural Networks](https://neurips.cc/virtual/2023/poster/70525)
 3D view point 
 ## $SE(3)$ Equivariant Convolution and Transformer in Ray Space [70778.png (4844×2710) (neurips.cc)](https://neurips.cc/media/PosterPDFs/NeurIPS%202023/70778.png?t=1701834127.32113)
 
privacy / distillation
[Students Parrot Their Teachers: Membership Inference on Model Distillation](https://neurips.cc/virtual/2023/poster/71209)


LLM-rlhf
alpaca farm [neurips.cc/media/neurips-2023/Slides/72842_eQCLdQJ.pdf](https://neurips.cc/media/neurips-2023/Slides/72842_eQCLdQJ.pdf)

Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision

optimizers
Memory Efficient Optimizers with 4-bit States [Memory Efficient Optimizers with 4-bit States Poster (nips.cc)](https://nips.cc/virtual/2023/poster/70507)
Convergence of Adam Under Relaxed Assumptions [Convergence of Adam Under Relaxed Assumptions Poster (neurips.cc)](https://neurips.cc/virtual/2023/poster/69959)
LLM
Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers [Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers Poster (neurips.cc)](https://neurips.cc/virtual/2023/poster/70129)

LLM quantization
QuIP: 2-Bit Quantization of Large Language Models With Guarantees [QuIP: 2-Bit Quantization of Large Language Models With Guarantees Poster (neurips.cc)](https://neurips.cc/virtual/2023/poster/69982)


Pre-RMSNorm and Pre-CRMSNorm Transformers: Equivalent and Efficient Pre-LN Transformers [Pre-RMSNorm and Pre-CRMSNorm Transformers: Equivalent and Efficient Pre-LN Transformers Poster (neurips.cc)](https://neurips.cc/virtual/2023/poster/69918)

prompting
Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective [NeurIPS 2023 Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective Oral](https://neurips.cc/virtual/2023/oral/73822)
Optimizing Prompts for Text-to-Image Generation [Optimizing Prompts for Text-to-Image Generation Poster (neurips.cc)](https://neurips.cc/virtual/2023/poster/72460)

Graph
Structure-free Graph Condensation: From Large-scale Graphs to Condensed Graph-free Data [Structure-free Graph Condensation: From Large-scale Graphs to Condensed Graph-free Data Poster (neurips.cc)](https://neurips.cc/virtual/2023/poster/71338)

CV- diffusion model 
Parallel Sampling of Diffusion Models [Parallel Sampling of Diffusion Models Poster (neurips.cc)](https://neurips.cc/virtual/2023/poster/71125)
Stable Diffusion is Unstable  [Stable Diffusion is Unstable Poster (neurips.cc)](https://neurips.cc/virtual/2023/poster/70194)
data mixture 

DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining [DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining Poster (neurips.cc)](https://neurips.cc/virtual/2023/poster/70588)
